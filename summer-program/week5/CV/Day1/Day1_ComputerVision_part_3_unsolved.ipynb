{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ReemAlsharabi/KAUST-Academy/blob/main/summer-program/week5/CV/Day1/Day1_ComputerVision_part_3_unsolved.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7aZ1iH90uC-b"
      },
      "source": [
        "# Part 3: Video Classification (2.5 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WwXxK7-TuHB6"
      },
      "source": [
        "There are many video datasets [available](https://datasetsearch.research.google.com/) for free online intended for research purposes, such as [YouTube-8M](https://research.google.com/youtube8m/) and [ActivityNet](http://activity-net.org/). We will be using the [YouTubeVideoGame](https://ai.googleblog.com/2013/11/released-data-set-features-extracted.html) dataset which contains multiview (multi-modal) hand-crafted video features (vision, text, and audio) extracted from 120K YouTube videos of people playing 30+ popular video games. The authors of this dataset hid the original YouTube video links. All what we have is just the extracted vision, text and audio features for each video (inputs) and the class label of the featured game (output). We are only interested in the vision features. You can get the dataset and learn more about it from [here](https://code.google.com/archive/p/multiview-video-features-data/wikis/InfoOnData.wiki).\n",
        "\n",
        "The vision modality has 5 feature families, similar to the audio modality, while the text has only 3. Combined, they are 13 feature families. Each feature family is an N-dimensional feature vector collected using some [feature extraction method](https://deepai.org/machine-learning-glossary-and-terms/feature-extraction) (e.g.,\n",
        "[HOG](https://en.wikipedia.org/wiki/Histogram_of_oriented_gradients) features). Every feature family was saved into a sperate compressed text file `*.txt.gz`. They also, split the training set from the validation from the testing set. Thus, we will create a dataset class for every feature family (called `YTVGFeatureFamily`) and create another dataset class combining feature families of the same split (train, validation, test) and modality (called `YouTubeVideoGame`).\n",
        "\n",
        "Skim through the following code-cell very quickly. It defines `YTVGFeatureFamily`. The most relavent parts are `__getitem__` and `__len__`. The former method is to give you one sample given its index and the later to return the total number of samples in the dataset. It also has a flag `by_video_id` to use video ids instead of indices. Most of these features have high sparsity (most values are zeros). Therefore, they opted to store the features in the sparse [COO(rdinate) format](https://pytorch.org/docs/1.7.1/sparse.html). However, there is no need to fret as it can be easly converted to the regular dense format by calling `Tensor.to_dense()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "gDda8poK-W9v"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import gzip\n",
        "from pathlib import Path\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "\n",
        "class YTVGFeatureFamily(Dataset):\n",
        "    \"\"\"YouTube Video Game Dataset (YTVG).\n",
        "\n",
        "    http://ai.googleblog.com/2013/11/released-data-set-features-extracted.html\n",
        "    http://code.google.com/archive/p/multiview-video-features-data/\n",
        "\n",
        "    YTVG dataset has 13 feature family divided into three modalities;\n",
        "    vision (5 families), audio (5 families), and text (3 families).\n",
        "\n",
        "    Args:\n",
        "        file_path: path to the *.txt.gz feature family file.\n",
        "        by_video_id: accessing items is by video_id instead of index.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    modalities = {\n",
        "        'vision': (\n",
        "            'cuboids_histogram',\n",
        "            'hs_hist_stream',\n",
        "            'hist_motion_estimate',\n",
        "            'misc',\n",
        "            'hog_features',\n",
        "        ),\n",
        "        'audio': (\n",
        "            'mfcc',\n",
        "            'sai_intervalgrams',\n",
        "            'sai_boxes',\n",
        "            'volume_stream',\n",
        "            'spectrogram_stream',\n",
        "        ),\n",
        "        'text': (\n",
        "            'description_unigrams',\n",
        "            'tag_unigrams',\n",
        "            'game_lda_1000',\n",
        "        ),\n",
        "    }\n",
        "\n",
        "    def __init__(self, file_path, by_video_id=False):\n",
        "        self.file_path = Path(file_path)\n",
        "\n",
        "        # whether to select items by video id or index\n",
        "        self.by_video_id = bool(by_video_id)\n",
        "\n",
        "        # placeholders for the data\n",
        "        self.index = {}  # key: video_id, value: index\n",
        "        self.video_id = []\n",
        "        self.class_label = []\n",
        "        self.indices = []\n",
        "        self.values = []\n",
        "\n",
        "        # read all the instances (videos) in the *.txt.gz file\n",
        "        with gzip.open(self.file_path, 'rt') as txt_gz:\n",
        "            instances = iter(txt_gz.read().split('#I'))\n",
        "            next(instances)\n",
        "\n",
        "        # parse each instace\n",
        "        for index, instance in enumerate(instances):\n",
        "            video_id, class_label, *features = instance.split()\n",
        "            if features:\n",
        "                indices, values = zip(*map(lambda x: x.split(':'), features))\n",
        "            else:\n",
        "                indices, values = [], []\n",
        "            indices = torch.LongTensor(tuple(map(int, indices))) - 1\n",
        "            values = torch.FloatTensor(tuple(map(float, values)))\n",
        "\n",
        "            self.index[int(video_id)] = index\n",
        "            self.video_id.append(int(video_id))\n",
        "            self.class_label.append(int(class_label) - 1)\n",
        "            self.indices.append(indices)\n",
        "            self.values.append(values)\n",
        "\n",
        "        # get the size of this feature (maximum among all instances)\n",
        "        self.size = max(i.max().item() for i in self.indices if len(i)) + 1\n",
        "\n",
        "    @property\n",
        "    def name(self):\n",
        "        return self.file_path.name.split('.')[0]\n",
        "\n",
        "    @property\n",
        "    def modality(self):\n",
        "        return self.file_path.name.split('_')[0]\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        if self.by_video_id:\n",
        "            index = self.index[index]\n",
        "        i = self.indices[index].unsqueeze(0)\n",
        "        v = self.values[index]\n",
        "        feature = torch.sparse.FloatTensor(i, v, (self.size,))\n",
        "        return {\n",
        "            'index': index,\n",
        "            'video_id': self.video_id[index],\n",
        "            'class_label': self.class_label[index],\n",
        "            'feature': feature,\n",
        "        }\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.video_id)\n",
        "\n",
        "    def __contains__(self, video_id):\n",
        "        return video_id in self.index\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f'{type(self).__name__}({self.name}_{self.size})'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOosAYMNoEpo"
      },
      "source": [
        "You cannot test the previous class before downloading and extracting its files. The following dataset class (`YouTubeVideoGame`) should be able to download these for you. If the automatic download failed, you can download them yourself using the links that you will get prompted. These are large files they could take around 2GB of RAM after they are loaded."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t9lJmaVUOoMp"
      },
      "outputs": [],
      "source": [
        "from torchvision.datasets.utils import extract_archive\n",
        "from torchvision.datasets.utils import download_file_from_google_drive\n",
        "\n",
        "\n",
        "class YouTubeVideoGame(Dataset):\n",
        "    \"\"\"YouTube Video Game Dataset (YTVG).\n",
        "\n",
        "    http://ai.googleblog.com/2013/11/released-data-set-features-extracted.html\n",
        "    http://code.google.com/archive/p/multiview-video-features-data/\n",
        "\n",
        "    Args:\n",
        "        data_dir: root directory for data files.\n",
        "        modality: must be in {'vision', 'audio', 'text'}.\n",
        "        split: must be in {'train', 'validation', 'test'}.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    file_id = {\n",
        "        # Google Drive file ids\n",
        "        'dir_vision': '0B4ZwSjYLbUK3Qmp0YWR2ckVKc2c',\n",
        "        'dir_audio1': '0B4ZwSjYLbUK3dFVTa0hvMGJxdXc',\n",
        "        'dir_audio2': '0B4ZwSjYLbUK3NTUyU0pQUDFpc3c',\n",
        "        'dir_text': '0B4ZwSjYLbUK3WnMwTW93a1Bmc28',\n",
        "        'validation': '0B4ZwSjYLbUK3ZDlZRG9pazZ6eGc',\n",
        "        'test': '0B4ZwSjYLbUK3SjJJX1UtSThBZnM',\n",
        "    }\n",
        "    md5 = {\n",
        "        # MD5 values for each file\n",
        "        'dir_vision': 'b8c5bc715405d526716008ee792589c0',\n",
        "        'dir_audio1': '5f10d11c2c601ff80e775c1a2ef361d3',\n",
        "        'dir_audio2': '7d3ca965c9f430451799b3d68ac51498',\n",
        "        'dir_text': '1e21c10b38df59b8c488f99607ff814e',\n",
        "        'validation': 'e975214da0ff36702b9e649c0ee32035',\n",
        "        'test': 'ae11b3769eb46c62cd72ddc8252694bf',\n",
        "    }\n",
        "\n",
        "    def __init__(self, data_dir=None, modality='vision', split='train'):\n",
        "        self.data_dir = data_dir\n",
        "        self.modality = modality\n",
        "        self.split = split\n",
        "\n",
        "        # select the correct data files\n",
        "        files = []\n",
        "        if self.split == 'train':\n",
        "            if self.modality == 'vision':\n",
        "                files.append('dir_vision')\n",
        "            elif self.modality == 'audio':\n",
        "                files.append('dir_audio1')\n",
        "                files.append('dir_audio2')\n",
        "            elif self.modality == 'text':\n",
        "                files.append('dir_text')\n",
        "        else:\n",
        "            files.append(self.split)\n",
        "\n",
        "        # download the files and load feature families\n",
        "        self.features = []\n",
        "        for tar in files:\n",
        "            self.download_and_extract(tar)\n",
        "            for name in YTVGFeatureFamily.modalities[self.modality]:\n",
        "                gz = self.data_dir / f'{tar}/{self.modality}_{name}.txt.gz'\n",
        "                print(f'loading {gz} ...')\n",
        "                feature = YTVGFeatureFamily(gz, by_video_id=True)\n",
        "                self.features.append(feature)\n",
        "\n",
        "    @classmethod\n",
        "    def default_dir(cls):\n",
        "        \"\"\"Get the default dataset files directory.\"\"\"\n",
        "        return Path(torch.hub.get_dir()) / f'datasets/{cls.__name__}'\n",
        "\n",
        "    @property\n",
        "    def data_dir(self):\n",
        "        \"\"\"Get dataset files directory.\"\"\"\n",
        "        return self._data_dir\n",
        "\n",
        "    @data_dir.setter\n",
        "    def data_dir(self, path):\n",
        "        if path is None:\n",
        "            path = self.default_dir()\n",
        "        self._data_dir = Path(path)\n",
        "\n",
        "    def download_and_extract(self, file_name):\n",
        "        \"\"\"Download and extract a dataset file.\"\"\"\n",
        "        directory = self.data_dir / file_name\n",
        "        path = self.data_dir / (file_name + '.tar')\n",
        "        if not directory.exists():\n",
        "            file_id = self.file_id[file_name]\n",
        "            print(f'did not find {str(directory)}')\n",
        "            print(f'downloading https://drive.google.com/file/d/{file_id}')\n",
        "            print(f'if it is taking more than expected, download it yourself')\n",
        "            print(f'you should then rename it and place it here {path}')\n",
        "            download_file_from_google_drive(\n",
        "                file_id=file_id,\n",
        "                root=self.data_dir,\n",
        "                filename=file_name + '.tar',\n",
        "                md5=self.md5[file_name],\n",
        "            )\n",
        "            extract_archive(str(path))\n",
        "\n",
        "    @property\n",
        "    def size(self):\n",
        "        return sum(f.size for f in self.features)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        video_id = self.features[0].video_id[index]\n",
        "        output = self.features[0][video_id]\n",
        "        output['feature'] = []\n",
        "        for feature in self.features:\n",
        "            if video_id in feature:\n",
        "                vector = feature[video_id]['feature']\n",
        "            else:\n",
        "                size = torch.Size([feature.size])\n",
        "                vector = torch.sparse.FloatTensor(size)\n",
        "            output['feature'].append(vector)\n",
        "        output['feature'] = torch.cat(output['feature'])\n",
        "        return output\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.features[0])\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f'{type(self).__name__}({self.modality}_{self.split})'\n",
        "\n",
        "\n",
        "# this may take a while to download and a while to load\n",
        "test_set = YouTubeVideoGame(modality='vision', split='test')\n",
        "val_set = YouTubeVideoGame(modality='vision', split='validation')\n",
        "train_set = YouTubeVideoGame(modality='vision', split='train')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9utVZ1azCrt"
      },
      "source": [
        "The dataset has 31 classes $[0, 30]$ (30 games + 1 unspecified). The unspecified class (background class) includes all videos of games that are not in the previous 30 classes. Unfortunately, the `__getitem__()` function returns a `dict`, while our implementation of `gradient_descent()` expects a `tuple` of two values; in input dense feature vector and an output class label. `YouTubeVideoGame` doesn't even offer a way to add transforms for each item. So, we will need to create a wrapper class for `YouTubeVideoGame` (called `YTVG` for short)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dNlpxVSov_ri"
      },
      "outputs": [],
      "source": [
        "# TODO: vvvvvvvvvvv\n",
        "# compelete the wrapper class\n",
        "# add small Gaussian noise to features if self.augment is True\n",
        "# comment the noise out if it hurts the generalizaiton\n",
        "class YTVG(Dataset):\n",
        "    def __init__(self, dataset, augment=False):\n",
        "        self.dataset = dataset\n",
        "        self.augment = augment\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        output = self.dataset[index]\n",
        "        class_label = ...\n",
        "        features = ...\n",
        "        if self.augment:\n",
        "            # TODO:\n",
        "            pass\n",
        "        return features, class_label\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __repr__(self):\n",
        "        return repr(self.dataset)\n",
        "# ^^^^^^^^^^^^^^^^^\n",
        "\n",
        "# wrap the datasets\n",
        "if isinstance(train_set, YouTubeVideoGame):\n",
        "    train_set = YTVG(train_set, augment=True)\n",
        "if isinstance(val_set, YouTubeVideoGame):\n",
        "    val_set = YTVG(val_set)\n",
        "if isinstance(test_set, YouTubeVideoGame):\n",
        "    test_set = YTVG(test_set)\n",
        "\n",
        "print(train_set, len(train_set))\n",
        "print(val_set, len(val_set))\n",
        "print(test_set, len(test_set))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9yjEIYtZ_laC"
      },
      "outputs": [],
      "source": [
        "# TODO: vvvvvvvvvvv\n",
        "# create the data loaders\n",
        "\n",
        "# ^^^^^^^^^^^^^^^^^"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g--D7G04y4HX"
      },
      "outputs": [],
      "source": [
        "# TODO: vvvvvvvvvvv\n",
        "# train a deep model on the vision modality of YouTubeVideoGame dataset\n",
        "# you must plot training and validation loss and accuracy per epoch\n",
        "# and report the final testing accuracy as we did in the previous parts\n",
        "# there is no target accuracy required feel free to experiments ;)\n",
        "torch.manual_seed(0)\n",
        "\n",
        "# ^^^^^^^^^^^^^^^^^"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WOFPAmNKLlso"
      },
      "source": [
        "## On Vision Transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gDuxkYorLlso"
      },
      "source": [
        "If there's a word that you heard too many times, it's transformers. In this project we are interested in exploring Vision Transformers (ViTs). In 2022, ViTs emerged as a competitive alternative to convolutional neural networks (CNNs). Transformer architecture was first introduced in natural language processing (NLP) and later was extended to image classification (and other vision tasks), check [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/pdf/2010.11929v2.pdf). Without going back and forth we directly jump into the various components of a ViT architecture.\n",
        "\n",
        "1. Split an image into patches (fixed sizes).\n",
        "\n",
        "2. Flatten the image patches.\n",
        "\n",
        "3. Create lower-dimensional linear embeddings from these flattened image patches.\n",
        "\n",
        "4. Include positional embeddings.\n",
        "\n",
        "5. Feed the sequence as an input to a state-of-the-art transformer encoder.\n",
        "\n",
        "6. Pre-train the ViT model with image labels, which is then fully supervised on a big dataset.\n",
        "\n",
        "7. Fine-tune the downstream dataset for image classification\n",
        "\n",
        "![Vision Transformers](https://viso.ai/wp-content/uploads/2021/09/vision-transformer-vit.png)\n",
        "\n",
        "Please read the following posts to get a better understanding of ViTs before proceeding to the below TODOs!\n",
        "\n",
        "* https://lilianweng.github.io/posts/2020-04-07-the-transformer-family/\n",
        "\n",
        "* https://jalammar.github.io/illustrated-transformer/\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xR7VzVmxLlsp"
      },
      "source": [
        "In each assignmnet we will have a ViT related task. Today's task is achieving steps 1 and 2, that is we need to first patchify the images then flatten them so we could later feed into an embedding layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TVo12IoyLlsp"
      },
      "outputs": [],
      "source": [
        "# patchify the image\n",
        "\n",
        "def convert_image_to_patches(image, patch_size):\n",
        "    \"\"\"\n",
        "    As discussed above, Vision Transformers Require Patchification of the input image.\n",
        "    This function takes an image tensor and returns a tensor of patches. (# Batches, # Patches, # Channels, Patch Size, Patch Size)\n",
        "\n",
        "    images - PyTorch tensor containing the images of a batch and has the shape [B, C, H, W]\n",
        "    patch_size - Number of pixels per dimension of the patches (integer)\n",
        "\n",
        "    expected output: PyTorch tensor of shape [B, N, C, patch_size, patch_size] where N is the number of patches obtained from the image after patchification.\n",
        "    \"\"\"\n",
        "\n",
        "    # TODO: fill in the blanks to patchify the image\n",
        "\n",
        "    B, C, H, W = x.shape\n",
        "\n",
        "    x = x.reshape()\n",
        "    x = x.permute()\n",
        "    x = x.flatten()\n",
        "\n",
        "    return x\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H00EsCZpLlsp"
      },
      "outputs": [],
      "source": [
        "# TODO: Visualize the patches in an image grid using torchvision.utils.make_grid or matplotlib subplots or a tool of your choice."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D9jh1FTzLlsp"
      },
      "outputs": [],
      "source": [
        "#TODO: Now we need to flatten the patches into a vector. A simple way to do that is use .flatten() function of PyTorch. We need to pass Batch x Patches x ??.\n",
        "# ?? represents the obtained vector dimension after flattening."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "vscode": {
      "interpreter": {
        "hash": "65dbb5abcda98b3ffc5f3a567b3b0bc593128e0efb7aa5031efe9548239084bb"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}